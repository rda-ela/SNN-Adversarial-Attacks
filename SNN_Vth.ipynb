{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "name": "notebook87b41093bc.ipynb",
      "provenance": []
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install norse==0.0.4\n",
        "!pip install torch==1.5.0+cu101 torchvision==0.6.0+cu101 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "!pip install foolbox==3.3.1"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-08-11T21:57:53.273361Z",
          "iopub.execute_input": "2022-08-11T21:57:53.274073Z",
          "iopub.status.idle": "2022-08-11T22:00:18.659810Z",
          "shell.execute_reply.started": "2022-08-11T21:57:53.273976Z",
          "shell.execute_reply": "2022-08-11T22:00:18.658617Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dVBw7Ldn9ry3",
        "outputId": "2d9cfa74-4ea2-49d9-d93d-3c01912eec7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting norse==0.0.4\n",
            "  Downloading norse-0.0.4-py3-none-any.whl (95 kB)\n",
            "\u001b[K     |████████████████████████████████| 95 kB 5.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from norse==0.0.4) (1.12.0+cu113)\n",
            "Requirement already satisfied: torchvision>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from norse==0.0.4) (0.13.0+cu113)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from norse==0.0.4) (1.2.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from norse==0.0.4) (1.21.6)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from norse==0.0.4) (3.2.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.5.0->norse==0.0.4) (4.1.1)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision>=0.6.0->norse==0.0.4) (7.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchvision>=0.6.0->norse==0.0.4) (2.23.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->norse==0.0.4) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->norse==0.0.4) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->norse==0.0.4) (1.4.4)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->norse==0.0.4) (3.0.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->norse==0.0.4) (1.15.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision>=0.6.0->norse==0.0.4) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision>=0.6.0->norse==0.0.4) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision>=0.6.0->norse==0.0.4) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision>=0.6.0->norse==0.0.4) (2022.6.15)\n",
            "Installing collected packages: norse\n",
            "Successfully installed norse-0.0.4\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Collecting torch==1.5.0+cu101\n",
            "  Downloading https://download.pytorch.org/whl/cu101/torch-1.5.0%2Bcu101-cp37-cp37m-linux_x86_64.whl (703.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 703.8 MB 23 kB/s \n",
            "\u001b[?25hCollecting torchvision==0.6.0+cu101\n",
            "  Downloading https://download.pytorch.org/whl/cu101/torchvision-0.6.0%2Bcu101-cp37-cp37m-linux_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 53.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from torch==1.5.0+cu101) (0.16.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.5.0+cu101) (1.21.6)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision==0.6.0+cu101) (7.1.2)\n",
            "Installing collected packages: torch, torchvision\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.12.0+cu113\n",
            "    Uninstalling torch-1.12.0+cu113:\n",
            "      Successfully uninstalled torch-1.12.0+cu113\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.13.0+cu113\n",
            "    Uninstalling torchvision-0.13.0+cu113:\n",
            "      Successfully uninstalled torchvision-0.13.0+cu113\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchtext 0.13.0 requires torch==1.12.0, but you have torch 1.5.0+cu101 which is incompatible.\n",
            "torchaudio 0.12.0+cu113 requires torch==1.12.0, but you have torch 1.5.0+cu101 which is incompatible.\n",
            "fastai 2.7.7 requires torch<1.13,>=1.7, but you have torch 1.5.0+cu101 which is incompatible.\n",
            "fastai 2.7.7 requires torchvision>=0.8.2, but you have torchvision 0.6.0+cu101 which is incompatible.\u001b[0m\n",
            "Successfully installed torch-1.5.0+cu101 torchvision-0.6.0+cu101\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting foolbox==3.3.1\n",
            "  Downloading foolbox-3.3.1-py3-none-any.whl (1.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.7 MB 33.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from foolbox==3.3.1) (1.7.3)\n",
            "Collecting GitPython>=3.0.7\n",
            "  Downloading GitPython-3.1.27-py3-none-any.whl (181 kB)\n",
            "\u001b[K     |████████████████████████████████| 181 kB 75.0 MB/s \n",
            "\u001b[?25hCollecting eagerpy==0.29.0\n",
            "  Downloading eagerpy-0.29.0-py3-none-any.whl (30 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from foolbox==3.3.1) (1.21.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from foolbox==3.3.1) (57.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.1 in /usr/local/lib/python3.7/dist-packages (from foolbox==3.3.1) (4.1.1)\n",
            "Collecting requests>=2.24.0\n",
            "  Downloading requests-2.28.1-py3-none-any.whl (62 kB)\n",
            "\u001b[K     |████████████████████████████████| 62 kB 1.9 MB/s \n",
            "\u001b[?25hCollecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 2.1 MB/s \n",
            "\u001b[?25hCollecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.24.0->foolbox==3.3.1) (2.10)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.24.0->foolbox==3.3.1) (2.1.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.24.0->foolbox==3.3.1) (2022.6.15)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.24.0->foolbox==3.3.1) (1.24.3)\n",
            "Installing collected packages: smmap, gitdb, requests, GitPython, eagerpy, foolbox\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.23.0\n",
            "    Uninstalling requests-2.23.0:\n",
            "      Successfully uninstalled requests-2.23.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchtext 0.13.0 requires torch==1.12.0, but you have torch 1.5.0+cu101 which is incompatible.\n",
            "fastai 2.7.7 requires torch<1.13,>=1.7, but you have torch 1.5.0+cu101 which is incompatible.\n",
            "fastai 2.7.7 requires torchvision>=0.8.2, but you have torchvision 0.6.0+cu101 which is incompatible.\u001b[0m\n",
            "Successfully installed GitPython-3.1.27 eagerpy-0.29.0 foolbox-3.3.1 gitdb-4.0.9 requests-2.28.1 smmap-5.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "from norse.torch.functional.lif import LIFParameters\n",
        "from norse.torch.module.leaky_integrator import LICell\n",
        "from norse.torch.module.lif import LIFFeedForwardCell\n",
        "\n",
        "import norse.torch.functional.encode as encode\n",
        "\n",
        "import foolbox as fb\n",
        "import foolbox.attacks as fa\n",
        "from foolbox import PyTorchModel, accuracy, samples\n",
        "\n",
        "import statistics\n",
        "\n",
        "\n",
        "epochs = 10\n",
        "\n",
        "#The values in the lists are the one we use as combinations\n",
        "\n",
        "#vth values\n",
        "#v_thh=[0.25,0.5,0.75,1.0,1.25,1.5,1.75,2.0,2.25]\n",
        "v_thh=[1.75]\n",
        "\n",
        "#T values\n",
        "# TT = [32,40,48,56,64,72,80]\n",
        "TT = [48]\n",
        "\n",
        "\n",
        "#We recommend using a GPU as applying the attack to the SNN model takes a lot of time\n",
        "\n",
        "device = torch.device(\"cuda\")\n",
        "mmodel = \"super\"\n",
        "lr = 0.0001\n",
        "input_features = 32 * 32\n",
        "\n",
        "batchsize = 64\n",
        "\n",
        "\n",
        "transform = transforms.Compose(\n",
        "    [transforms.Resize(32),\n",
        "     transforms.ToTensor(),\n",
        "     torchvision.transforms.Normalize((0.1307,), (0.3081,))])\n",
        "\n",
        "trainset = torchvision.datasets.MNIST(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "train_loader = torch.utils.data.DataLoader(trainset, batch_size=batchsize,\n",
        "                                          shuffle=True)\n",
        "\n",
        "testset = torchvision.datasets.MNIST(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "test_loader = torch.utils.data.DataLoader(testset, batch_size=batchsize,\n",
        "                                         shuffle=False)\n",
        "\n",
        "\n",
        "class IFConstantCurrentEncoder(torch.nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        seq_length,\n",
        "        tau_mem_inv=1.0 / 1e-2,\n",
        "        v_th=1.0,\n",
        "        v_reset=0.0,\n",
        "        dt: float = 0.001,\n",
        "    ):\n",
        "        super(IFConstantCurrentEncoder, self).__init__()\n",
        "        self.seq_length = seq_length\n",
        "        self.tau_mem_inv = tau_mem_inv\n",
        "        self.v_th = v_th\n",
        "        self.v_reset = v_reset\n",
        "        self.dt = dt\n",
        "\n",
        "    def forward(self, x):\n",
        "        lif_parameters = LIFParameters(tau_mem_inv=self.tau_mem_inv, v_th=self.v_th, v_reset=self.v_reset)\n",
        "        return encode.constant_current_lif_encode(x, self.seq_length, p=lif_parameters, dt=self.dt)\n",
        "\n",
        "\n",
        "class ConvvNet4(torch.nn.Module):\n",
        "    def __init__(\n",
        "        self, device, num_channels=1, feature_size=32, method=\"super\", dtype=torch.float\n",
        "    ):\n",
        "        super(ConvvNet4, self).__init__()\n",
        "        self.features = int(((feature_size - 4) / 2 - 4) / 2)\n",
        "\n",
        "        self.conv1 = torch.nn.Conv2d(1, 6, kernel_size=5, stride=1)\n",
        "        self.conv2 = torch.nn.Conv2d(6, 16, kernel_size=5,stride=1)\n",
        "        self.conv3 = torch.nn.Conv2d(16, 120, kernel_size=5, stride=1)\n",
        "        self.fc1 = torch.nn.Linear(120, 84)\n",
        "#         self.fc2 = torch.nn.Linear(84, 10)\n",
        "\n",
        "        self.lif0 = LIFFeedForwardCell(p=LIFParameters(method=method, alpha=100.0))\n",
        "        self.lif1 = LIFFeedForwardCell(p=LIFParameters(method=method, alpha=100.0))\n",
        "        self.lif2 = LIFFeedForwardCell(p=LIFParameters(method=method, alpha=100.0))\n",
        "        self.lif3 = LIFFeedForwardCell(p=LIFParameters(method=method, alpha=100.0))\n",
        "        self.out = LICell(84, 10)\n",
        "\n",
        "        self.device = device\n",
        "        self.dtype = dtype\n",
        "\n",
        "    def forward(self, x):\n",
        "        seq_length = x.shape[0]\n",
        "        batch_size = x.shape[1]\n",
        "\n",
        "        # specify the initial states\n",
        "        s0 = None\n",
        "        s1 = None\n",
        "        s2 = None\n",
        "        s3 = None\n",
        "        so = None\n",
        "\n",
        "        voltages = torch.zeros(\n",
        "            seq_length, batch_size, 10, device=self.device, dtype=self.dtype\n",
        "        )\n",
        "\n",
        "        for ts in range(seq_length):\n",
        "            z = self.conv1(x[ts, :])\n",
        "            z, s0 = self.lif0(z, s0)\n",
        "            z = torch.nn.functional.max_pool2d(torch.nn.functional.relu(z), 2, 2)\n",
        "            z = 10 * self.conv2(z)\n",
        "            z, s1 = self.lif1(z, s1)\n",
        "            z = torch.nn.functional.max_pool2d(torch.nn.functional.relu(z), 2, 2)\n",
        "            z = 10 * self.conv3(z)\n",
        "            z, s2 = self.lif2(z, s2)\n",
        "            z = torch.nn.functional.relu(z)\n",
        "#           z = z.view(-1, 16*5*5)\n",
        "            z = torch.flatten(z, 1)\n",
        "            z = self.fc1(z)\n",
        "            z, s3 = self.lif3(z, s3)\n",
        "            v, so = self.out(torch.nn.functional.relu(z), so)\n",
        "            voltages[ts, :, :] = v\n",
        "        return voltages\n",
        "\n",
        "\n",
        "def train(model, device, train_loader, optimizer, epoch, writer=None):\n",
        "    model.train()\n",
        "    losses = []\n",
        "\n",
        "    batch_len = len(train_loader)\n",
        "    step = batch_len * epoch\n",
        "\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = torch.nn.functional.nll_loss(output, target)\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "        step += 1\n",
        "\n",
        "        if batch_idx % 100 == 0:\n",
        "            print(\n",
        "                \"Train Epoch: {}/{} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
        "                    epoch,\n",
        "                    epochs,\n",
        "                    batch_idx * len(data),\n",
        "                    len(train_loader.dataset),\n",
        "                    100.0 * batch_idx / len(train_loader),\n",
        "                    loss.item(),\n",
        "                )\n",
        "            )\n",
        "\n",
        "        losses.append(loss.item())\n",
        "\n",
        "    mean_loss = np.mean(losses)\n",
        "    return losses, mean_loss\n",
        "\n",
        "\n",
        "def test(model, device, test_loader, epoch, writer=None):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += torch.nn.functional.nll_loss(\n",
        "                output, target, reduction=\"sum\"\n",
        "            ).item()  # sum up batch loss\n",
        "            pred = output.argmax(\n",
        "                dim=1, keepdim=True\n",
        "            )  # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "\n",
        "    taccuracy = 100.0 * correct / len(test_loader.dataset)\n",
        "    print(\n",
        "        f\"\\nTest set {mmodel}: Average loss: {test_loss:.4f}, \\\n",
        "            Accuracy: {correct}/{len(test_loader.dataset)} ({taccuracy:.0f}%)\\n\"\n",
        "    )\n",
        "\n",
        "    return test_loss, taccuracy\n",
        "\n",
        "\n",
        "class LIFConvNet(torch.nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_features,\n",
        "        seq_length,\n",
        "        v_th,\n",
        "        model=\"super\",\n",
        "        only_first_spike=False,\n",
        "    ):\n",
        "        super(LIFConvNet, self).__init__()\n",
        "        self.constant_current_encoder = IFConstantCurrentEncoder(seq_length=seq_length,v_th=v_th)\n",
        "        self.only_first_spike = only_first_spike\n",
        "        self.input_features = input_features\n",
        "        self.rsnn = ConvvNet4(method=model,device=device)\n",
        "        self.seq_length = seq_length\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size = x.shape[0]\n",
        "        x = self.constant_current_encoder(\n",
        "            x.view(-1, self.input_features) * 1\n",
        "        )\n",
        "        if self.only_first_spike:\n",
        "            # delete all spikes except for first\n",
        "            zeros = torch.zeros_like(x.cpu()).detach().numpy()\n",
        "            idxs = x.cpu().nonzero().detach().numpy()\n",
        "            spike_counter = np.zeros((batchsize, 32 * 32))\n",
        "            for t, batch, nrn in idxs:\n",
        "                if spike_counter[batch, nrn] == 0:\n",
        "                    zeros[t, batch, nrn] = 1\n",
        "                    spike_counter[batch, nrn] += 1\n",
        "            x = torch.from_numpy(zeros).to(x.device)\n",
        "\n",
        "        x = x.reshape(self.seq_length, batch_size, 1, 32, 32)\n",
        "        voltages = self.rsnn(x)\n",
        "        m, _ = torch.max(voltages, 0)\n",
        "        log_p_y = torch.nn.functional.log_softmax(m, dim=1)\n",
        "        return log_p_y\n",
        "\n",
        "\n",
        "#This function does the same work as foolbox, it allows us to compare the values of foolbox\n",
        "#It's only used to check if there is no issues with the model given to foolbox\n",
        "\n",
        "def benchmark():\n",
        "    listeimg = []\n",
        "    listeadv = []\n",
        "    for i in range(len(images)):\n",
        "      imaj = images[i]\n",
        "      img = imaj\n",
        "      listeimg.append(img)\n",
        "      tempadvlist = []\n",
        "      for e in range(len(epsilons)):\n",
        "        iadvs = advs[e][i]\n",
        "        adv = iadvs\n",
        "        tempadvlist.append(adv)\n",
        "      listeadv.append(tempadvlist)\n",
        "\n",
        "    results = []\n",
        "\n",
        "    model.eval()\n",
        "    for e in range(len(epsilons)):\n",
        "      tempepsilons = []\n",
        "      for i in range(len(listeadv)):\n",
        "        imgadvtest = listeadv[i][e]\n",
        "        lb = labels[i].item()\n",
        "        with torch.no_grad():\n",
        "          output = model(imgadvtest.unsqueeze(0))\n",
        "        pred = output.data.max(1, keepdim=True)[1][0].item()\n",
        "        prob = torch.nn.functional.softmax(output, dim=1)\n",
        "        top_p, top_class = prob.topk(1, dim = 1)\n",
        "        if(str(lb)==str(pred)):\n",
        "          tempepsilons.append(\"FALSE\")\n",
        "        else:\n",
        "          tempepsilons.append(\"TRUE\")\n",
        "      results.append(tempepsilons)\n",
        "\n",
        "    for i in range(len(results)):\n",
        "        v = 0\n",
        "        for e in range(len(results[0])):\n",
        "            if results[i][e] == \"FALSE\":\n",
        "                v = v+1\n",
        "        print(\"eps: \"+ str(epsilons[i]), end=' | ')\n",
        "        print(\"pred en %: \" + str(float(v/len(results[0]))*100))\n",
        "        print(\"\")\n",
        "\n",
        "\n",
        "for i in range(len(TT)):\n",
        "    for j in range(len(v_thh)):\n",
        "\n",
        "        print(TT[i],v_thh[j])\n",
        "        print(\" \")\n",
        "\n",
        "        T = TT[i]\n",
        "        v = v_thh[j]\n",
        "\n",
        "        model = LIFConvNet(\n",
        "            input_features=input_features,\n",
        "            seq_length=T,\n",
        "            v_th=v,\n",
        "#             model=mmodel,\n",
        "#             only_first_spike=False,\n",
        "        ).to(device)\n",
        "\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "        training_losses = []\n",
        "        mean_losses = []\n",
        "        test_losses = []\n",
        "        accuracies = []\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "                training_loss, mean_loss = train(model, device, train_loader, optimizer, epoch)\n",
        "                test_loss, taccuracy = test(model, device, test_loader, epoch)\n",
        "\n",
        "                training_losses += training_loss\n",
        "                mean_losses.append(mean_loss)\n",
        "                test_losses.append(test_loss)\n",
        "                accuracies.append(taccuracy)\n",
        "\n",
        "                max_accuracy = np.max(np.array(accuracies))\n",
        "\n",
        "        adv_loader = torch.utils.data.DataLoader(testset, batch_size=batchsize,\n",
        "                                                 shuffle=False)\n",
        "\n",
        "        examples = enumerate(adv_loader)\n",
        "        batch_idx, (images, labels) = next(examples)\n",
        "\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # print(model)\n",
        "\n",
        "        fmodel = fb.PyTorchModel(model, bounds=(0, 1))\n",
        "\n",
        "        print(\" \")\n",
        "        print(\"accuracy\", end=' | ')\n",
        "        print(accuracy(fmodel, images, labels))\n",
        "        print(\"\")\n",
        "\n",
        "        attacks = [fb.attacks.PGD()]\n",
        "\n",
        "        epsilons = [\n",
        "            0.0,\n",
        "#             0.1,\n",
        "#             0.2,\n",
        "#             0.3,\n",
        "#             0.4,\n",
        "#             0.5,\n",
        "#             0.6,\n",
        "#             0.7,\n",
        "#             0.8,\n",
        "#             0.9,\n",
        "#             1.0,\n",
        "#             1.1,\n",
        "#             1.2,\n",
        "#             1.3,\n",
        "#             1.4,\n",
        "#             1.5,\n",
        "#             1.6,\n",
        "#             1.7,\n",
        "#             1.8,\n",
        "#             1.9,\n",
        "#             2.0,\n",
        "        ]\n",
        "\n",
        "        print(\"epsilons\")\n",
        "        print(epsilons)\n",
        "        print(\"\")\n",
        "\n",
        "        lbyat = []\n",
        "        fulatac = []\n",
        "#         for i, attack in enumerate(attacks):\n",
        "#             _, advs, success = attack(fmodel, images, labels, epsilons=epsilons)\n",
        "#             benchmark()\n",
        "#             atacc = []\n",
        "#             robust_accuracy = 1 - success.float().mean(axis=-1)\n",
        "#             for eps, acc in zip(epsilons, robust_accuracy):\n",
        "#                 print(attack, eps, acc.item())\n",
        "#                 atacc.append(acc.item())\n",
        "#             print(\" \")\n",
        "#             fulatac.append(atacc)\n",
        "#             lfull = []\n",
        "#             for e in range(len(epsilons)):\n",
        "#                 l = []\n",
        "#                 for i in range(len(images)):\n",
        "#                     perturbation = advs[e][i][0].cpu().numpy() - images[i][0].cpu().numpy()\n",
        "#                     l.append(float(format(np.linalg.norm(perturbation.flatten()))))\n",
        "#                 lfull.append(statistics.mean(l))\n",
        "#             lbyat.append(lfull)\n"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "execution": {
          "iopub.status.busy": "2022-08-11T22:03:29.280976Z",
          "iopub.execute_input": "2022-08-11T22:03:29.281353Z",
          "iopub.status.idle": "2022-08-11T22:07:29.059370Z",
          "shell.execute_reply.started": "2022-08-11T22:03:29.281306Z",
          "shell.execute_reply": "2022-08-11T22:07:29.058233Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VodPjHGO9ry6",
        "outputId": "78f519b5-f836-4df3-c906-babb6f6592f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "48 1.75\n",
            " \n",
            "Train Epoch: 0/10 [0/60000 (0%)]\tLoss: 2.290102\n",
            "Train Epoch: 0/10 [6400/60000 (11%)]\tLoss: 2.285015\n",
            "Train Epoch: 0/10 [12800/60000 (21%)]\tLoss: 2.265591\n",
            "Train Epoch: 0/10 [19200/60000 (32%)]\tLoss: 2.199225\n",
            "Train Epoch: 0/10 [25600/60000 (43%)]\tLoss: 2.135959\n",
            "Train Epoch: 0/10 [32000/60000 (53%)]\tLoss: 1.979241\n",
            "Train Epoch: 0/10 [38400/60000 (64%)]\tLoss: 1.756737\n",
            "Train Epoch: 0/10 [44800/60000 (75%)]\tLoss: 1.655881\n",
            "Train Epoch: 0/10 [51200/60000 (85%)]\tLoss: 1.525823\n",
            "Train Epoch: 0/10 [57600/60000 (96%)]\tLoss: 1.325927\n",
            "\n",
            "Test set super: Average loss: 1.3432,             Accuracy: 6770/10000 (68%)\n",
            "\n",
            "Train Epoch: 1/10 [0/60000 (0%)]\tLoss: 1.376179\n",
            "Train Epoch: 1/10 [6400/60000 (11%)]\tLoss: 1.184985\n",
            "Train Epoch: 1/10 [12800/60000 (21%)]\tLoss: 1.117057\n",
            "Train Epoch: 1/10 [19200/60000 (32%)]\tLoss: 1.054947\n",
            "Train Epoch: 1/10 [25600/60000 (43%)]\tLoss: 0.887813\n",
            "Train Epoch: 1/10 [32000/60000 (53%)]\tLoss: 0.850268\n",
            "Train Epoch: 1/10 [38400/60000 (64%)]\tLoss: 0.869950\n",
            "Train Epoch: 1/10 [44800/60000 (75%)]\tLoss: 0.635365\n",
            "Train Epoch: 1/10 [51200/60000 (85%)]\tLoss: 0.627330\n",
            "Train Epoch: 1/10 [57600/60000 (96%)]\tLoss: 0.612646\n",
            "\n",
            "Test set super: Average loss: 0.5935,             Accuracy: 8592/10000 (86%)\n",
            "\n",
            "Train Epoch: 2/10 [0/60000 (0%)]\tLoss: 0.512677\n",
            "Train Epoch: 2/10 [6400/60000 (11%)]\tLoss: 0.497980\n",
            "Train Epoch: 2/10 [12800/60000 (21%)]\tLoss: 0.507200\n",
            "Train Epoch: 2/10 [19200/60000 (32%)]\tLoss: 0.584200\n",
            "Train Epoch: 2/10 [25600/60000 (43%)]\tLoss: 0.610665\n",
            "Train Epoch: 2/10 [32000/60000 (53%)]\tLoss: 0.529770\n",
            "Train Epoch: 2/10 [38400/60000 (64%)]\tLoss: 0.444785\n",
            "Train Epoch: 2/10 [44800/60000 (75%)]\tLoss: 0.589887\n",
            "Train Epoch: 2/10 [51200/60000 (85%)]\tLoss: 0.453804\n",
            "Train Epoch: 2/10 [57600/60000 (96%)]\tLoss: 0.450308\n",
            "\n",
            "Test set super: Average loss: 0.4163,             Accuracy: 8849/10000 (88%)\n",
            "\n",
            "Train Epoch: 3/10 [0/60000 (0%)]\tLoss: 0.449434\n",
            "Train Epoch: 3/10 [6400/60000 (11%)]\tLoss: 0.529150\n",
            "Train Epoch: 3/10 [12800/60000 (21%)]\tLoss: 0.329918\n",
            "Train Epoch: 3/10 [19200/60000 (32%)]\tLoss: 0.334679\n",
            "Train Epoch: 3/10 [25600/60000 (43%)]\tLoss: 0.394038\n",
            "Train Epoch: 3/10 [32000/60000 (53%)]\tLoss: 0.220990\n",
            "Train Epoch: 3/10 [38400/60000 (64%)]\tLoss: 0.474573\n",
            "Train Epoch: 3/10 [44800/60000 (75%)]\tLoss: 0.315312\n",
            "Train Epoch: 3/10 [51200/60000 (85%)]\tLoss: 0.415823\n",
            "Train Epoch: 3/10 [57600/60000 (96%)]\tLoss: 0.351323\n",
            "\n",
            "Test set super: Average loss: 0.3331,             Accuracy: 9022/10000 (90%)\n",
            "\n",
            "Train Epoch: 4/10 [0/60000 (0%)]\tLoss: 0.323773\n",
            "Train Epoch: 4/10 [6400/60000 (11%)]\tLoss: 0.238717\n",
            "Train Epoch: 4/10 [12800/60000 (21%)]\tLoss: 0.612854\n",
            "Train Epoch: 4/10 [19200/60000 (32%)]\tLoss: 0.352734\n",
            "Train Epoch: 4/10 [25600/60000 (43%)]\tLoss: 0.274607\n",
            "Train Epoch: 4/10 [32000/60000 (53%)]\tLoss: 0.259545\n",
            "Train Epoch: 4/10 [38400/60000 (64%)]\tLoss: 0.348226\n",
            "Train Epoch: 4/10 [44800/60000 (75%)]\tLoss: 0.530786\n",
            "Train Epoch: 4/10 [51200/60000 (85%)]\tLoss: 0.227475\n",
            "Train Epoch: 4/10 [57600/60000 (96%)]\tLoss: 0.326241\n",
            "\n",
            "Test set super: Average loss: 0.2873,             Accuracy: 9138/10000 (91%)\n",
            "\n",
            "Train Epoch: 5/10 [0/60000 (0%)]\tLoss: 0.315878\n",
            "Train Epoch: 5/10 [6400/60000 (11%)]\tLoss: 0.245297\n",
            "Train Epoch: 5/10 [12800/60000 (21%)]\tLoss: 0.383608\n",
            "Train Epoch: 5/10 [19200/60000 (32%)]\tLoss: 0.438907\n",
            "Train Epoch: 5/10 [25600/60000 (43%)]\tLoss: 0.237156\n",
            "Train Epoch: 5/10 [32000/60000 (53%)]\tLoss: 0.219213\n",
            "Train Epoch: 5/10 [38400/60000 (64%)]\tLoss: 0.357382\n",
            "Train Epoch: 5/10 [44800/60000 (75%)]\tLoss: 0.280863\n",
            "Train Epoch: 5/10 [51200/60000 (85%)]\tLoss: 0.420312\n",
            "Train Epoch: 5/10 [57600/60000 (96%)]\tLoss: 0.165126\n",
            "\n",
            "Test set super: Average loss: 0.2562,             Accuracy: 9249/10000 (92%)\n",
            "\n",
            "Train Epoch: 6/10 [0/60000 (0%)]\tLoss: 0.271353\n",
            "Train Epoch: 6/10 [6400/60000 (11%)]\tLoss: 0.303030\n",
            "Train Epoch: 6/10 [12800/60000 (21%)]\tLoss: 0.352668\n",
            "Train Epoch: 6/10 [19200/60000 (32%)]\tLoss: 0.174821\n",
            "Train Epoch: 6/10 [25600/60000 (43%)]\tLoss: 0.318116\n",
            "Train Epoch: 6/10 [32000/60000 (53%)]\tLoss: 0.210309\n",
            "Train Epoch: 6/10 [38400/60000 (64%)]\tLoss: 0.165286\n",
            "Train Epoch: 6/10 [44800/60000 (75%)]\tLoss: 0.106159\n",
            "Train Epoch: 6/10 [51200/60000 (85%)]\tLoss: 0.182719\n",
            "Train Epoch: 6/10 [57600/60000 (96%)]\tLoss: 0.155415\n",
            "\n",
            "Test set super: Average loss: 0.2339,             Accuracy: 9296/10000 (93%)\n",
            "\n",
            "Train Epoch: 7/10 [0/60000 (0%)]\tLoss: 0.272052\n",
            "Train Epoch: 7/10 [6400/60000 (11%)]\tLoss: 0.172856\n",
            "Train Epoch: 7/10 [12800/60000 (21%)]\tLoss: 0.184110\n",
            "Train Epoch: 7/10 [19200/60000 (32%)]\tLoss: 0.125183\n",
            "Train Epoch: 7/10 [25600/60000 (43%)]\tLoss: 0.213210\n",
            "Train Epoch: 7/10 [32000/60000 (53%)]\tLoss: 0.171077\n",
            "Train Epoch: 7/10 [38400/60000 (64%)]\tLoss: 0.332894\n",
            "Train Epoch: 7/10 [44800/60000 (75%)]\tLoss: 0.186449\n",
            "Train Epoch: 7/10 [51200/60000 (85%)]\tLoss: 0.293030\n",
            "Train Epoch: 7/10 [57600/60000 (96%)]\tLoss: 0.232387\n",
            "\n",
            "Test set super: Average loss: 0.2088,             Accuracy: 9345/10000 (93%)\n",
            "\n",
            "Train Epoch: 8/10 [0/60000 (0%)]\tLoss: 0.090523\n",
            "Train Epoch: 8/10 [6400/60000 (11%)]\tLoss: 0.227137\n",
            "Train Epoch: 8/10 [12800/60000 (21%)]\tLoss: 0.314904\n",
            "Train Epoch: 8/10 [19200/60000 (32%)]\tLoss: 0.156997\n",
            "Train Epoch: 8/10 [25600/60000 (43%)]\tLoss: 0.197845\n",
            "Train Epoch: 8/10 [32000/60000 (53%)]\tLoss: 0.131596\n",
            "Train Epoch: 8/10 [38400/60000 (64%)]\tLoss: 0.145253\n",
            "Train Epoch: 8/10 [44800/60000 (75%)]\tLoss: 0.129347\n",
            "Train Epoch: 8/10 [51200/60000 (85%)]\tLoss: 0.137492\n",
            "Train Epoch: 8/10 [57600/60000 (96%)]\tLoss: 0.285401\n",
            "\n",
            "Test set super: Average loss: 0.1918,             Accuracy: 9423/10000 (94%)\n",
            "\n",
            "Train Epoch: 9/10 [0/60000 (0%)]\tLoss: 0.100936\n",
            "Train Epoch: 9/10 [6400/60000 (11%)]\tLoss: 0.127775\n",
            "Train Epoch: 9/10 [12800/60000 (21%)]\tLoss: 0.127578\n",
            "Train Epoch: 9/10 [19200/60000 (32%)]\tLoss: 0.141769\n",
            "Train Epoch: 9/10 [25600/60000 (43%)]\tLoss: 0.172004\n",
            "Train Epoch: 9/10 [32000/60000 (53%)]\tLoss: 0.208114\n",
            "Train Epoch: 9/10 [38400/60000 (64%)]\tLoss: 0.261733\n",
            "Train Epoch: 9/10 [44800/60000 (75%)]\tLoss: 0.389430\n",
            "Train Epoch: 9/10 [51200/60000 (85%)]\tLoss: 0.174683\n",
            "Train Epoch: 9/10 [57600/60000 (96%)]\tLoss: 0.150984\n",
            "\n",
            "Test set super: Average loss: 0.1798,             Accuracy: 9464/10000 (95%)\n",
            "\n",
            " \n",
            "accuracy | 1.0\n",
            "\n",
            "epsilons\n",
            "[0.0]\n",
            "\n"
          ]
        }
      ]
    }
  ]
}